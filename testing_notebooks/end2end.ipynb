{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 23:08:01 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from distilabel.llms import vLLM\n",
    "from distilabel.pipeline import Pipeline\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import hf_hub_download\n",
    "from vllm import LLM\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prompts generated: 1014\n",
      "\n",
      "Sample prompts:\n",
      "- What does The Fool reveal about relationship flow in my Relationships?\n",
      "- What does The Fool reveal about resource in my Wealth?\n",
      "- What does The Fool reveal about future in my Career?\n",
      "- What does The Fool reveal about block in my Self-Growth?\n",
      "- What does The Fool reveal about present in my Health?\n"
     ]
    }
   ],
   "source": [
    "card_names = [\n",
    "    'The Fool',\n",
    "    'The Magician',\n",
    "    'The High Priestess',\n",
    "    'The Empress',\n",
    "    'The Emperor',\n",
    "    'The Hierophant',\n",
    "    'The Lovers',\n",
    "    'The Chariot',\n",
    "    'Strength',\n",
    "    'The Hermit',\n",
    "    'Wheel of Fortune',\n",
    "    'Justice',\n",
    "    'The Hanged Man',\n",
    "    'Death',\n",
    "    'Temperance',\n",
    "    'The Devil',\n",
    "    'The Tower',\n",
    "    'The Star',\n",
    "    'The Moon',\n",
    "    'The Sun',\n",
    "    'Judgment',\n",
    "    'The World',\n",
    "    'Ace of Wands',\n",
    "    'Two of Wands',\n",
    "    'Three of Wands',\n",
    "    'Four of Wands',\n",
    "    'Five of Wands',\n",
    "    'Six of Wands',\n",
    "    'Seven of Wands',\n",
    "    'Eight of Wands',\n",
    "    'Nine of Wands',\n",
    "    'Ten of Wands',\n",
    "    'Page of Wands',\n",
    "    'Knight of Wands',\n",
    "    'Queen of Wands',\n",
    "    'King of Wands',\n",
    "    'Ace of Cups',\n",
    "    'Two of Cups',\n",
    "    'Three of Cups',\n",
    "    'Four of Cups',\n",
    "    'Five of Cups',\n",
    "    'Six of Cups',\n",
    "    'Seven of Cups',\n",
    "    'Eight of Cups',\n",
    "    'Nine of Cups',\n",
    "    'Ten of Cups',\n",
    "    'Page of Cups',\n",
    "    'Knight of Cups',\n",
    "    'Queen of Cups',\n",
    "    'King of Cups',\n",
    "    'Ace of Swords',\n",
    "    'Two of Swords',\n",
    "    'Three of Swords',\n",
    "    'Four of Swords',\n",
    "    'Five of Swords',\n",
    "    'Six of Swords',\n",
    "    'Seven of Swords',\n",
    "    'Eight of Swords',\n",
    "    'Nine of Swords',\n",
    "    'Ten of Swords',\n",
    "    'Page of Swords',\n",
    "    'Knight of Swords',\n",
    "    'Queen of Swords',\n",
    "    'King of Swords',\n",
    "    'Ace of Pentacles',\n",
    "    'Two of Pentacles',\n",
    "    'Three of Pentacles',\n",
    "    'Four of Pentacles',\n",
    "    'Five of Pentacles',\n",
    "    'Six of Pentacles',\n",
    "    'Seven of Pentacles',\n",
    "    'Eight of Pentacles',\n",
    "    'Nine of Pentacles',\n",
    "    'Ten of Pentacles',\n",
    "    'Page of Pentacles',\n",
    "    'Knight of Pentacles',\n",
    "    'Queen of Pentacles',\n",
    "    'King of Pentacles',\n",
    "]\n",
    "\n",
    "topics = {\n",
    "    \"Relationships\": [\"their mind\", \"relationship flow\", \"your move\", \"challenge\", \"solution\", \"evolution\"],\n",
    "    \"Wealth\": [\"current\", \"future\", \"opportunity\", \"resource\", \"action\", \"reward\"],\n",
    "    \"Career\": [\"now\", \"challenge\", \"future\", \"skill\", \"potential\", \"opportunity\"],\n",
    "    \"Self-Growth\": [\"now\", \"block\", \"potential\", \"mind\", \"spirit\", \"guide\"],\n",
    "    \"Health\": [\"past\", \"present\", \"future\", \"challenge\", \"advice\"],\n",
    "    \"Spiritual Guidance\": [\"past\", \"present\", \"future\", \"challenge\", \"advice\"],\n",
    "    \"Life Purpose\": [\"past\", \"present\", \"future\", \"challenge\", \"advice\"],\n",
    "    \"Daily Guidance\": [\"event\", \"emotion\", \"reflection\"],\n",
    "    \"Weekly Guidance\": [\"event\", \"emotion\", \"reflection\"]\n",
    "}\n",
    "\n",
    "# Define main topics and guidance topics\n",
    "main_topics = [\"Relationships\", \"Wealth\", \"Career\", \"Self-Growth\", \"Health\", \"Spiritual Guidance\", \"Life Purpose\"]\n",
    "guidance_topics = [\"Daily Guidance\", \"Weekly Guidance\"]\n",
    "\n",
    "# Function to generate prompt for main topics\n",
    "def generate_main_prompt(card, topic, positional_meanings):\n",
    "    pos_meaning = random.choice(positional_meanings)\n",
    "    return f\"What does {card} reveal about {pos_meaning} in my {topic}?\"\n",
    "\n",
    "# Function to generate prompts for guidance topics\n",
    "def generate_guidance_prompts(card, topic):\n",
    "    prompts = []\n",
    "    for pos_meaning in topics[topic]:\n",
    "        prompts.append(f\"What {pos_meaning} does {card} suggest for my {topic.lower()}?\")\n",
    "    return prompts\n",
    "\n",
    "# Generate all prompts\n",
    "prompts = []\n",
    "for card in card_names:\n",
    "    # Generate prompts for main topics (1 prompt per topic per card)\n",
    "    for topic in main_topics:\n",
    "        prompt = generate_main_prompt(card, topic, topics[topic])\n",
    "        prompts.append(prompt)\n",
    "    # Generate prompts for guidance topics (3 prompts per topic per card)\n",
    "    for topic in guidance_topics:\n",
    "        guidance_prompts = generate_guidance_prompts(card, topic)\n",
    "        prompts.extend(guidance_prompts)\n",
    "\n",
    "# Output the total number of prompts and a sample\n",
    "print(f\"Total prompts generated: {len(prompts)}\")\n",
    "print(\"\\nSample prompts:\")\n",
    "for prompt in prompts[:5]:\n",
    "    print(f\"- {prompt}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distilabel.models import InferenceEndpointsLLM\n",
    "from distilabel.pipeline import Pipeline\n",
    "from distilabel.steps import (\n",
    "    LoadDataFromHub,\n",
    "    GroupColumns,\n",
    "    FormatTextGenerationDPO,\n",
    "    PreferenceToArgilla,\n",
    ")\n",
    "\n",
    "from distilabel.steps.tasks import TextGeneration, UltraFeedback\n",
    "\n",
    "\n",
    "generate_responses = [\n",
    "    TextGeneration(\n",
    "        llm=InferenceEndpointsLLM(\n",
    "            model_id=\"sft_awq\",\n",
    "            tokenizer_id=\"sft_awq\",\n",
    "            generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "        ),\n",
    "        pipeline=Pipeline(name=\"showcase-pipeline\"),\n",
    "    ),\n",
    "    TextGeneration(\n",
    "        llm=InferenceEndpointsLLM(\n",
    "            model_id=\"sft_qwen\",\n",
    "            tokenizer_id=\"sft_qwen\",\n",
    "            generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "        ),\n",
    "        pipeline=Pipeline(name=\"showcase-pipeline\"),\n",
    "    ),\n",
    "]\n",
    "# for task in generate_responses:\n",
    "#     task.load()\n",
    "\n",
    "\n",
    "group_responses = GroupColumns(\n",
    "    columns=[\"generation\", \"model_name\"],\n",
    "    output_columns=[\"generations\", \"model_names\"],\n",
    "    pipeline=Pipeline(name=\"showcase-pipeline\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distilabel.steps.tasks import UltraFeedback\n",
    "from distilabel.models import InferenceEndpointsLLM\n",
    "\n",
    "evaluate_responses = UltraFeedback(\n",
    "    aspect=\"overall-rating\",\n",
    "    # aspect=\"helpfulness\"\n",
    "    llm=InferenceEndpointsLLM(\n",
    "        model_id=\"Qwen/QwQ-32B\",\n",
    "        tokenizer_id=\"Qwen/QwQ-32B\",\n",
    "        generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "    ),\n",
    "    pipeline=Pipeline(name=\"showcase-pipeline\"),\n",
    ")\n",
    "\n",
    "format_dpo = FormatTextGenerationDPO(pipeline=Pipeline(name=\"showcase-pipeline\"))\n",
    "format_dpo.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distilabel.steps import Step, StepInput\n",
    "from distilabel.pipeline import Pipeline\n",
    "from typing import List\n",
    "\n",
    "class AdjustRatingsStep(Step):\n",
    "    alpha: float = 0.1  # Configurable hyperparameter\n",
    "\n",
    "    def process(self, inputs: StepInput) -> List[dict]:\n",
    "        for row in inputs:\n",
    "            lengths = [len(gen) for gen in row[\"generations\"]]\n",
    "            penalties = [self.alpha * length for length in lengths]\n",
    "            adjusted_ratings = [rating - penalty for rating, penalty in zip(row[\"ratings\"], penalties)]\n",
    "            row[\"ratings\"] = adjusted_ratings\n",
    "        return inputs\n",
    "\n",
    "# Define the pipeline\n",
    "with Pipeline(name=\"showcase-pipeline\") as pipeline:\n",
    "    adjust_ratings = AdjustRatingsStep()\n",
    "    format_dpo = FormatTextGenerationDPO()\n",
    "    adjust_ratings >> format_dpo  # Connect steps\n",
    "\n",
    "# Run the pipeline\n",
    "dataset = [\n",
    "    {\n",
    "        \"instruction\": \"What's the capital of Spain?\",\n",
    "        \"generations\": [\"Madrid\", \"Barcelona\"],\n",
    "        \"generation_models\": [\"Meta-Llama-3-8B-Instruct\", \"Mixtral-8x7B-Instruct-v0.1\"],\n",
    "        \"ratings\": [5, 1],\n",
    "    }\n",
    "]\n",
    "result = pipeline.run(inputs=dataset)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "dataset = [\n",
    "    {\n",
    "        \"instruction\": \"What's the capital of Spain?\",\n",
    "        \"generations\": [\"Madrid\", \"Barcelona\"],\n",
    "        \"generation_models\": [\"Meta-Llama-3-8B-Instruct\", \"Mixtral-8x7B-Instruct-v0.1\"],\n",
    "        \"ratings\": [5, 1],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to adjust ratings based on length\n",
    "def adjust_ratings(row, alpha=0.1):\n",
    "    lengths = [len(gen) for gen in row[\"generations\"]]  # Length in characters\n",
    "    penalties = [alpha * length for length in lengths]\n",
    "    adjusted_ratings = [rating - penalty for rating, penalty in zip(row[\"ratings\"], penalties)]\n",
    "    row[\"ratings\"] = adjusted_ratings  # Overwrite original ratings\n",
    "    return row\n",
    "\n",
    "# Apply the adjustment\n",
    "adjusted_dataset = [adjust_ratings(row, alpha=0.1) for row in dataset]\n",
    "\n",
    "# Process with FormatTextGenerationDPO\n",
    "format_dpo = FormatTextGenerationDPO(pipeline=Pipeline(name=\"showcase-pipeline\"))\n",
    "format_dpo.load()\n",
    "result = next(format_dpo.process(adjusted_dataset))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
