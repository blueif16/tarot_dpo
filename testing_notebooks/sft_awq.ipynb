{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "\n",
    "# base_model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# base_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "\n",
    "# base_model = \"bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF\"\n",
    "# base_model = \"enferAI/DeepSeek-R1-Distill-Qwen-14B-FP8\"\n",
    "\n",
    "base_model = \"Qwen/QwQ-32B-AWQ\"\n",
    "# base_model = \"unsloth/DeepSeek-R1-Distill-Qwen-14B-bnb-4bit\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "# print(model.get_memory_footprint())\n",
    "\n",
    "# dataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n",
    "\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open(\"tarot_qa_finetuning_dataset_cleaned.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "dataset = Dataset.from_list(dataset)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(len(dataset['question']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \"bos_token\": null,\n",
    "  \"chat_template\": \"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- '' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n  {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" and not message.tool_calls %}\\n        {%- set content = message.content %}\\n        {%- if not loop.last %}\\n            {%- set content = message.content.split('</think>')[-1].lstrip('\\\\n') %}\\n        {%- endif %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {%- set content = message.content %}\\n        {%- if not loop.last %}\\n            {%- set content = message.content.split('</think>')[-1].lstrip('\\\\n') %}\\n        {%- endif %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n<think>\\\\n' }}\\n{%- endif %}\\n\",\n",
    "  \"clean_up_tokenization_spaces\": false,\n",
    "  \"eos_token\": \"<|im_end|>\",\n",
    "  \"errors\": \"replace\",\n",
    "  \"model_max_length\": 131072,\n",
    "  \"pad_token\": \"<|endoftext|>\",\n",
    "  \"split_special_tokens\": false,\n",
    "  \"tokenizer_class\": \"Qwen2Tokenizer\",\n",
    "  \"unk_token\": null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "\n",
    "    system_prompt = \"You are a tarot reading assistant, your goal is to generate the best response to user's choosen card in 4 sentences\"\n",
    "    for i in range(len(example['question'])):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": example['question'][i]},\n",
    "            {\"role\": \"assistant\", \"content\": example['answer'][i]}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "# response_template = \" ### Answer:\"\n",
    "response_template = \"<|im_start|>assistant\\n\"  # specifically for \n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=bnb_config)\n",
    "print(model.get_memory_footprint())\n",
    "\n",
    "peft_config = LoraConfig(r=32,\n",
    "                        lora_alpha=64,\n",
    "                        lora_dropout=0.05,\n",
    "                        bias=\"none\",\n",
    "                        task_type=\"CAUSAL_LM\"\n",
    "                      )\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "def main():\n",
    "    # parser = argparse.ArgumentParser(description=\"Training arguments\", allow_abbrev=False)\n",
    "\n",
    "\n",
    "    # parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of epochs\")\n",
    "    # parser.add_argument(\"--batch_size\", type=int, default=help=\"Batch size\")\n",
    "\n",
    "    # # Parse arguments\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    epoch = 30 if args.epochs is None else args.epochs\n",
    "    batch_size = 2 if args.batch_size is None else args.batch_size\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir='qwen_sft_4b_v2',\n",
    "        warmup_steps=1,\n",
    "        num_train_epochs=5, # adjust based on the data size\n",
    "        per_device_train_batch_size=4, # use 4 if you have more GPU RAM\n",
    "        gradient_accumulation_steps=4,\n",
    "        save_strategy=\"epoch\", #steps\n",
    "        logging_steps=100,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        learning_rate=2.5e-5,\n",
    "        fp16=True,\n",
    "        seed=42,\n",
    "        # save_steps=50,  # Save checkpoints every 50 steps\n",
    "        do_eval=False,   \n",
    "        )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        args=args,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        data_collator=collator,\n",
    "        peft_config=peft_config\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    new_model = \"sft_awq\"\n",
    "\n",
    "    trainer.model.save_pretrained(new_model)\n",
    "    trainer.tokenizer.save_pretrained(new_model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
