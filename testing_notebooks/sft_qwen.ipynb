{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35ca04de45645d38b520b21ef5bc2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917e597edae548d68e85ee714247f3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b435bdb484714b80b8688a65fd8bea91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3900\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "\n",
    "# base_model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# base_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "\n",
    "# base_model = \"bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF\"\n",
    "# base_model = \"enferAI/DeepSeek-R1-Distill-Qwen-14B-FP8\"\n",
    "\n",
    "base_model = \"unsloth/DeepSeek-R1-Distill-Qwen-14B-bnb-4bit\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "# print(model.get_memory_footprint())\n",
    "\n",
    "# dataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n",
    "\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open(\"tarot_qa_finetuning_dataset_cleaned.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "dataset = Dataset.from_list(dataset)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(len(dataset['question']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model requirement\n",
    "\n",
    "\"bos_token\": \"<｜begin▁of▁sentence｜>\",\n",
    "  \"chat_template\": \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\\\n'}}{% endif %}\",\n",
    "  \"clean_up_tokenization_spaces\": false,\n",
    "  \"eos_token\": \"<｜end▁of▁sentence｜>\",\n",
    "  \"extra_special_tokens\": {},\n",
    "  \"legacy\": true,\n",
    "  \"model_max_length\": 131072,\n",
    "  \"pad_token\": \"<|vision_pad|>\",\n",
    "  \"padding_side\": \"left\",\n",
    "  \"sp_model_kwargs\": {},\n",
    "  \"tokenizer_class\": \"LlamaTokenizerFast\",\n",
    "  \"unk_token\": null,\n",
    "  \"use_default_system_prompt\": false\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9a8d7335024046b634e61c81852ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m\n\u001b[1;32m     22\u001b[0m compute_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     25\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mcompute_dtype\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mget_memory_footprint())\n\u001b[1;32m     34\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     35\u001b[0m                         lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     36\u001b[0m                         lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[1;32m     37\u001b[0m                         bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m                         task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m                       )\n",
      "File \u001b[0;32m~/miniconda3/envs/fight/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/fight/lib/python3.11/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/fight/lib/python3.11/site-packages/transformers/modeling_utils.py:4319\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4314\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4315\u001b[0m     )\n\u001b[1;32m   4317\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m _get_resolved_checkpoint_files(\n\u001b[1;32m   4318\u001b[0m     pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39mpretrained_model_name_or_path,\n\u001b[0;32m-> 4319\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m   4320\u001b[0m     variant\u001b[38;5;241m=\u001b[39mvariant,\n\u001b[1;32m   4321\u001b[0m     gguf_file\u001b[38;5;241m=\u001b[39mgguf_file,\n\u001b[1;32m   4322\u001b[0m     from_tf\u001b[38;5;241m=\u001b[39mfrom_tf,\n\u001b[1;32m   4323\u001b[0m     from_flax\u001b[38;5;241m=\u001b[39mfrom_flax,\n\u001b[1;32m   4324\u001b[0m     use_safetensors\u001b[38;5;241m=\u001b[39muse_safetensors,\n\u001b[1;32m   4325\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   4326\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   4327\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   4328\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   4329\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   4330\u001b[0m     user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m   4331\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   4332\u001b[0m     commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m   4333\u001b[0m )\n\u001b[1;32m   4335\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4336\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fight/lib/python3.11/site-packages/transformers/modeling_utils.py:4897\u001b[0m, in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m   4882\u001b[0m     \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[1;32m   4883\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[1;32m   4884\u001b[0m         disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4885\u001b[0m             model_to_load,\n\u001b[1;32m   4886\u001b[0m             state_dict,\n\u001b[1;32m   4887\u001b[0m             shard_file,\n\u001b[1;32m   4888\u001b[0m             expected_keys,\n\u001b[1;32m   4889\u001b[0m             reverse_key_renaming_mapping,\n\u001b[1;32m   4890\u001b[0m             device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4891\u001b[0m             disk_offload_folder\u001b[38;5;241m=\u001b[39mdisk_offload_folder,\n\u001b[1;32m   4892\u001b[0m             disk_offload_index\u001b[38;5;241m=\u001b[39mdisk_offload_index,\n\u001b[1;32m   4893\u001b[0m             cpu_offload_folder\u001b[38;5;241m=\u001b[39mcpu_offload_folder,\n\u001b[1;32m   4894\u001b[0m             cpu_offload_index\u001b[38;5;241m=\u001b[39mcpu_offload_index,\n\u001b[1;32m   4895\u001b[0m             hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[1;32m   4896\u001b[0m             is_safetensors\u001b[38;5;241m=\u001b[39mis_offloaded_safetensors,\n\u001b[0;32m-> 4897\u001b[0m             keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   4898\u001b[0m             unexpected_keys\u001b[38;5;241m=\u001b[39munexpected_keys,\n\u001b[1;32m   4899\u001b[0m             device_mesh\u001b[38;5;241m=\u001b[39mdevice_mesh,\n\u001b[1;32m   4900\u001b[0m         )\n\u001b[1;32m   4901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4902\u001b[0m     assign_params \u001b[38;5;241m=\u001b[39m check_support_param_buffer_assignment(model_to_load, state_dict)\n",
      "File \u001b[0;32m~/miniconda3/envs/fight/lib/python3.11/site-packages/transformers/modeling_utils.py:896\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    884\u001b[0m         weights_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weights_name\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_resolved_checkpoint_files\u001b[39m(\n\u001b[1;32m    889\u001b[0m     pretrained_model_name_or_path: Optional[Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike]],\n\u001b[1;32m    890\u001b[0m     subfolder: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    891\u001b[0m     variant: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    892\u001b[0m     gguf_file: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    893\u001b[0m     from_tf: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    894\u001b[0m     from_flax: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    895\u001b[0m     use_safetensors: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m--> 896\u001b[0m     cache_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    897\u001b[0m     force_download: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    898\u001b[0m     proxies: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m    899\u001b[0m     local_files_only: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    900\u001b[0m     token: Optional[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m]],\n\u001b[1;32m    901\u001b[0m     user_agent: \u001b[38;5;28mdict\u001b[39m,\n\u001b[1;32m    902\u001b[0m     revision: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    903\u001b[0m     commit_hash: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    904\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[List[\u001b[38;5;28mstr\u001b[39m]], Optional[Dict]]:\n\u001b[1;32m    905\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get all the checkpoint filenames based on `pretrained_model_name_or_path`, and optional metadata if the\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;124;03m    checkpoints are sharded.\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;124;03m    This function will download the data if necesary.\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     is_sharded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fight/lib/python3.11/site-packages/accelerate/utils/modeling.py:329\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    327\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 329\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    # for i in range(len(example['question'])):\n",
    "    #     text = f\"### Question: {example['question'][i]}\\n ### Answer: {example['answer'][i]}\"\n",
    "    #     output_texts.append(text)\n",
    "    # return output_texts\n",
    "    system_prompt = \"You are a tarot reading assistant, your goal is to generate the best response to user's choosen card in 4 sentences\"\n",
    "    for i in range(len(example['question'])):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": example['question'][i]},\n",
    "            {\"role\": \"assistant\", \"content\": example['answer'][i]}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "# response_template = \" ### Answer:\"\n",
    "response_template = \"<｜Assistant｜>\"  # specifically for qwen\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=bnb_config)\n",
    "print(model.get_memory_footprint())\n",
    "\n",
    "peft_config = LoraConfig(r=32,\n",
    "                        lora_alpha=64,\n",
    "                        lora_dropout=0.05,\n",
    "                        bias=\"none\",\n",
    "                        task_type=\"CAUSAL_LM\"\n",
    "                      )\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "def main():\n",
    "    # parser = argparse.ArgumentParser(description=\"Training arguments\", allow_abbrev=False)\n",
    "\n",
    "\n",
    "    # parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of epochs\")\n",
    "    # parser.add_argument(\"--batch_size\", type=int, default=help=\"Batch size\")\n",
    "\n",
    "    # # Parse arguments\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    epoch = 30 if args.epochs is None else args.epochs\n",
    "    batch_size = 2 if args.batch_size is None else args.batch_size\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir='qwen_sft_4b_v2',\n",
    "        warmup_steps=1,\n",
    "        num_train_epochs=5, # adjust based on the data size\n",
    "        per_device_train_batch_size=4, # use 4 if you have more GPU RAM\n",
    "        gradient_accumulation_steps=4,\n",
    "        save_strategy=\"epoch\", #steps\n",
    "        logging_steps=100,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        learning_rate=2.5e-5,\n",
    "        fp16=True,\n",
    "        seed=42,\n",
    "        # save_steps=50,  # Save checkpoints every 50 steps\n",
    "        do_eval=False,   \n",
    "        )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        args=args,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        data_collator=collator,\n",
    "        peft_config=peft_config\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
